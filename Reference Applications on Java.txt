Slide 1
I would like to talk a bit about Java implementations of Databricks Reference
Applications for Spark.
First, why do we need Java implementations of these applications? We have them
already on Scala. Obviously, because Java is one of the most widely used
programming languages. On this chart - it is taken from a Stack Overflow survey -
you can see a list of most popular technologies. Predictably, Java is in this
list, and it is not a surprise that we don't find Scala among most popular
programming languages. A lot of commercial software is implemented with Java. Many
software companies employ Java programmers.

Scala is less popular, in part because it is much more difficult to master. So we
see a natural barrier that prevents many companies from using Spark. Spark has
Java API, which is less capable than the Scala API, but is more accessible. This
is why we need Java implementations of Reference Applications for Spark: Java
versions of Spark Reference Applications is an important driver that enables many
enterprises to begin to use Spark without embarking on a huge learning curve.

Slide 2
Currently, only the Log Analyzer application has a Java implementation. Log Analyzer
is written on Java 8 and works with Spark 2.0. Because Spark is written in a
functional programming style, older versions of Java are more difficult to work
with. The other reference applications, such as Twitter Language Classifier and
Weather Time Series are currently only available for Scala.

Slide 3
The Log Analyzer Reference Application is a guidebook accompanied with code
examples. Chapters 1-3 of the guidebook give information on how to use basic Spark
features, such as computations and data manipulations. Chapter 4 combines
knowledge from the previous 3 chapters to assemble the Log Analyzer application.
Log Analyzer processes Apache web server log files and produces statistics about
the web server traffic. It is a simple but fully functional application; you can
use it as a starting point to implement your commercial enterprise software with
Spark written on Java.

Slide 4
Log Analyzer produces statistics on request size, including: 
 - Minimal, maximal and average request size.
 - Counts of different response codes. 
 - Clients that have accessed this server more than 10 times. 
 - The top 10 popular URLs. 
The Log analyzer application takes real-time data as input by polling a directory
for new log files and calculates statistics over sliding windows as well as total
statistics for the total running time.

Slide 5
Log Analyzer demonstrates the usage of a variety of Spark features, including:
 - Streaming data
 - Directory polling for reading log files.
 - Sliding window calculations and cumulative state update, using the window() and
   updateStateByKey() methods.
 - Spark transformations like map(), filter()
 - Spark computations like reduce() and so on.

Slide 6
Additionally, the guidebook demonstrates the following Spark features:
 - Spark SQL API usage: how to use Dataset and DataFrame, how to execute SQL queries over
   structured data.
 - There are good DRY guidelines, including reusing existing code with bulk and
   streaming computations written as Java 8 lambda expressions. 
 - Obtaining results from Spark, including writing a Resilient Distributed Dataset to a
   directory on a distributed filesystem.

Slide 7
Scala is one of the most loved and top paying technologies. Spark specialists are
paid even highe. Java is neither among most loved technologies nor among top
paying technologies, but businesses can often get done what they need to with
Java. More advanced requirements will require the full usage of the Spark API, and
the only language that can fully exercise the Spark API is Scala. The 80/20 rule
probably applies to most businesses: Java is good enough to satisfy most
requirements, using the programming talent already in house. Save Scala for the
more demanding requirements.

Thank you.
